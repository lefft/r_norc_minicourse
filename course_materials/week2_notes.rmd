---
title: "R mini-course: week 2 notes"
author: "timothy leffel  &nbsp;&nbsp;&nbsp;&nbsp; may26/2017"
output: html_document
css: css/notes.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results="hold")
```

<hr style="height:2px; background-color: gray; color:gray;">

Same format as last week: exercises and additional discussion interleaved throughout. But this time, I'd recommend starting a blank R script and typing your responses into it. That way if you want feedback, you can just send me the `.R` file with your solutions (where applicable), including commented lines that explain what you're doing and why you're doing it (example [here](LINK TO SCREENSHOT OF AN EXAMPLE)). I'll send you back a file that has all of your original code, plus some comments and code from me. 

Also for next week, I want everyone (who's so compelled) to identify a dataset 
they'd be interested in exploring, and save it to the directory you're storing 
your files for this class in. It's definitely best if the data is in .csv format, 
but it doesn't have to be. Before the regular notes, here's a quick guide on why 
and how to find a dataset of interest. 

<br>

### 0. for next week: everyone obtain a dataset

<hr style="height:1px; background-color:lightgray; color:lightgray;">

Since R was designed to be used for data analysis, you'll only understand why R works the way it does -- and more importantly, what it can do for you -- by practicing writing R code that manipulates rectangular datasets. 

The best way to learn R is to use R. In my experience, the best way to motivate 
yourself to use R is to identify a topic that (i) has some kind of quantitative 
data associated with it; that (ii) you know how to find a dataset pertaining to; and that (iii) you have a *genuine* interest in understanding at a deeper level. 

Since we all work in social science, there's lots of candidates for (i) and (ii) -- even if one doesn't come to mind, ask a few colleagues and I'm sure someone will 
have some thoughts. 
*But* criterion (iii) can be harder to satisfy. Even so, I think it's crucial -- 
you'll get bored of and/or frustrated with playing around with `mtcars` and 
`iris` reeeeeal quickly (*trust me*). 

I'd actually go as far as to argue that when it 
comes to learning how to code in R, (iii) is *more* important than points (i) and (ii), because if you have a topic that satisfies (iii) but neither of the first two, then you can spend time 
thinking about how to code up simulations to model the phenomenon you're interested in. At the same time I'm sure everyone has a *genuine* interest in getting done with their work faster (more 
time for twitter :p), so a good dataset might be something work-related that 
satisfies (iii) in virtue of you being interested in finishing your work faster 
and consequently having more free time. 

Fortunately, the internet has many, many datasets to offer. For a shocking array 
of topics, you can google "datasets on X" and find interesting spreadsheet-like 
sets of numbers that can be read into R and manipulated just like `mtcars` or `iris` (in case you weren't in class: these datasets are introduced in section 1 below). 


Here's some genres you might consider: 

* spreadsheets that represent repeated measurements over time, or a set of spreadsheets that you could in theory merge together, e.g.
    + a project budget/expenditures table (or a collection of them); 
    + stock or currency trading prices; 
    + game-level, player-level sports data (e.g. box scores); 
    + longitudinal data from an experimental(y) study; or 
    + your personal budget!
* a set of survey responses your team has collected as part of a project
* a word list with fields for frequency, collocation, part-of-speech, etc. 
* some government data about labor, income, health, healthcare, etc. -- here's links to [the Census Bureau](LINK) and [the Bureau of Labor Statistics](LINK) for inspiration
<!-- end of list -->

Here's some questions to keep in mind when selecting a dataset:

* does it seem manageable to get acquainted with over the next few weeks? (e.g. a full GSS dataset might be too much to digest)
* will gaining familiarity with the dataset be beneficial to you, whether personally or professionally? 
* are you going to lose interest in the subject matter of the dataset quickly?
* does it have a reasonable size for quick work-checking? (let's say: between 3 and 20 columns of interest, and between 10 and 10,000 rows)


### 1. Working with real data

<hr style="height:1px; background-color:lightgray; color:lightgray;">


R comes with some pre-loaded datasets, which have reserved names. The two examples 
we'll look at here are `mtcars` and `iris`. They're both kinda famous datasets 
in the data science world, and you should be aware of them because a lot of 
tutorials and walkthroughs you'll find on the web use one of them as an example. `mtcars` is *Motor Trend* data from the '70s about various attributes of different types of cars. `iris` provides several measurements of 150 individual iris flowers, each of which belongs to one of three sub-species (spoiler: the measurements can be used to predict the sub-species reasonably well). This dataset was made famous by Ronald Fisher in the '30s and is still used regularly in examples. 

Most of the built-in datasets are **data frames**. You can access built-in datasets by their names:

```{r}
head(iris, n=5)
```

```{r}
head(mtcars, n=5)
```


Let's focus on `mtcars`. Notice that it's not in your environment tab. That's 
because there's a bunch of built-in datasets and it'd be annoying if they were 
all there all the time. We can just introduce a variable and assign the dataset 
to it:

```{r}
tim_mtcars <- mtcars
```

<!-- **exercise**: from a fresh(-ish) session, try this with e.g. `myvar <- myvar`. Why can't we do this with `myvar`? What does the error message tell you, and why doesn't it arise in the case of `mtcars`? What happens if you assign `myvar` a value first? -->

Let's check out what the columns are:

```{r}
str(tim_mtcars)
```

Not super informative. But if we google around a bit, we'll [find](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html) what each of 
them are: 

- `mtcars$mpg` -- miles per gallon
- `mtcars$cyl` -- number of cylinders
- `mtcars$disp` -- displacement (in$^3$)
- `mtcars$hp` -- gross horsepower
- `mtcars$drat` -- rear axle ratio 
- `mtcars$wt` -- weight (1000lb)
- `mtcars$qsec` -- 1/4 mile time
- `mtcars$vs` -- V/S (V- versus Straight block, I think)
- `mtcars$am` -- automatic or manual transmission
- `mtcars$gear` -- number of gears
- `mtcars$carb` -- number of carburetors

**exercise**: some of these columns are coded horribly. How could we improve 
the legibility of the data by transforming some of the columns? (hint: inspect and contemplate the column `mtcars$am`)


But wait, something important is missing: what are the makes and models of the cars?! Well, turns out they're in...

```{r}
rownames(tim_mtcars)
```

This is annoying, and in 2017 it's not very common to actually use rownames in R. Since 
`rownames(tim_mtcars)` is a character vector, we can just move it to a column and then delete the rownames.

```{r}
tim_mtcars$make_model <- rownames(tim_mtcars)
rownames(tim_mtcars) <- NULL
```

**note**: we can combine assignment and `$` with `NULL` to remove a column from a data frame. 

Do we have any missing values?

```{r}
# one way to check would be:
sum(is.na(tim_mtcars$mpg))
sum(is.na(tim_mtcars$cyl))
sum(is.na(tim_mtcars$disp))
# ...
```

```{r}
# a quicker way to check:
colSums(is.na(tim_mtcars))

"" # adding an empty string to make space between the two results in the output

# aaand make sure there aren't NA's that accidentally became characters
# (note "NA" is not the same as NA)
colSums(tim_mtcars=="NA")
```

**exercise**: use the `%in%` function to see whether the column `mtcars$wt` has any missing values. Note that `%in%` has **infix** syntax like `+`, `-`, `*`, and `/`, so we write `x %in% y` to see if `y` contains `x` (we'll get a logical back -- `TRUE` if `x` is in `y`, and `FALSE` otherwise). We'll see another useful infix operator below...

Let's clean up one of the columns, `am`, before moving on. Manual vs automatic 
should really not be coded as 0 or 1, since transmission is most definitely 
a category and not a number. We can use `ifelse()` to change this.

```{r}
tim_mtcars$am <- ifelse(tim_mtcars$am==0, "automatic", "manual")
```

**exercise**: it is *very* important that we checked missing values before recoding the `am` column in the way that we did. Why? What could happen if the thing we're recoding has missing values in it, but we go ahead and recode it with `ifelse()` anyway? How could we safely recode the column in the presence of missing values? 

```{r}
# now let's remove tim_mtcars since we want to manipulate the original later on
rm(tim_mtcars)
```


<br>

### 2. A brief but necessary detour: packages!

<hr style="height:1px; background-color:lightgray; color:lightgray;">

Very important point: **in R, packages are your friend**. 

One of the best things about R is that anyone can contribute helfpul, 
new user-defined functions and methods. These are typically bundled into a portable format called a **package**. Packages are just collections of functions and other stuff that aren't part 
of what automatically comes with R when you install it (aka **base R**).

If you are using a particular package for the first time, you will
have to install it, which is done with `install.packages("<package name>")`
(note quotes around the name). Everyone should install the following packages for the class: 

```{r}
# install.packages("dplyr")
# install.packages("reshape2")
# install.packages("ggplot2")
```

**note**: once you have a package installed, it stays installed until you update 
your version of R. Then, to reinstall a package, just call `install.packages()` on it again. 

After a package is installed, you can "load" it (i.e. make its functions available for use) with `library("<packagename>")`. For this course, we'll use the following packages (maybe more too).

```{r message=FALSE, warning=FALSE, results='hold'}
# don't worry if you get some output here that you don't expect!
# some packages send you messages when you load them. no need for concern. 
library("dplyr")
library("reshape2")
library("ggplot2")
```

You can see your **library** -- a list of your installed packages -- by saying
`library()`, *without* an argument. You can see which packages are currently **attached** ("loaded") with `search()`, again with no argument. 

```{r}
# see installed packages (will be different for everyone)
# library()

# see packages available *in current session*
search()
```

**note**: R Studio has lots of point-and-click tools to deal with package 
management and data import. Look at the [R Studio IDE cheatsheet](LINK) on the course page for details. 

**note**: Check out [CRAN](LINK TO CRAN) to browse tons of cool R packages, which exist for almost any imaginable purpose! 

<br>

### 3. The outside world (or: reading and writing external files)

<hr style="height:1px; background-color:lightgray; color:lightgray;">


#### 3.1 read from a url

Here's a cool word-frequency dataset: 

```{r}
# link to url of a word frequency dataset
link <- "http://lefft.xyz/r_minicourse/datasets/top5k-word-frequency-dot-info.csv"
# read in the dataset with defaults (header=TRUE, sep=",")
words <- read.csv(link)
# look at the first few rows
head(words, n=5)
```

**exercise**: plot frequency against rank in the `words` dataset. What does the pattern suggest? (*after* you do this, google "Zipf's Law")


#### 3.2 read from a local file

Here's a government education dataset I found [here](https://inventory.data.gov/dataset/032e19b4-5a90-41dc-83ff-6e4cd234f565/resource/38625c3d-5388-4c16-a30f-d105432553a4).

```{r}
# i saved it to a local folder, so I can read it in like this
edu_data <- read.csv("datasets/university/postscndryunivsrvy2013dirinfo.csv")
head(edu_data[, 1:10], n=5)
names(edu_data)
```


#### 3.3 reading different file types

Check out the `readxl::` package, and specifically try `readxl_example()` to see some examples. See [this vignette](https://cran.r-project.org/web/packages/readxl/vignettes/cell-and-column-types.html) for some examples of reading excel files in R. 


```{r results='markup'}
library("readxl")
# an example of reading xls datasets
crime1 <- read_xls("datasets/crime/Crime2016EXCEL/noncampusarrest131415.xls")
crime2 <- read_xls("datasets/crime/Crime2016EXCEL/noncampuscrime131415.xls")

# see how many rows + columns each one has
dim(crime1); dim(crime2)

# see what columns they have in common
intersect(names(crime1), names(crime2))

# looks like we could prooobably merge them by UNITID_P, which looks like an id
length(intersect(unique(crime1$UNITID_P), unique(crime2$UNITID_P)))

# could also join by INSTNM
identical(crime1$INSTNM, crime2$INSTNM)

# suppose we want to join these together, matching by UNITID_P
# here's one way to do it (note that left_join() comes from dplyr::):
crime <- left_join(crime1, crime2, by=c("UNITID_P", "INSTNM"))

# now if we check it out, it's kinda a mess but it worked. 
# this suggests we should've inspected the columns more thoroughly 
# before joining them (bc we've probably duplicated some matching columns)
```

**exercise**: look at the [data wrangling cheatsheet](LINK), and determine 
whether `left_join()` was the best strategy to use here, given the crime data. 
How else could we have joined the datasets? What would make a left join *not* 
give us what we want?

Here's [a link](http://www.stata.com/links/examples-and-datasets/) to some datasets in Stata format.
```{r}
# an example of reading a stata dta file (note we need the haven:: package)
library("haven")
election_data <- read_dta("datasets/election/bes_f2f_original_v3.0.dta")

# notice that objects read from stata maintain some of their 
# idiosyncratic internal structure -- e.g. you can see the survey items 
# "embedded" inside the header fields in the R Studio spreadsheet view 
head(election_data, n=5)
```

We can even extract descriptions of each column, since they are present in the original `.dta` file:

```{r}
# how many columns are there
numcols <- ncol(election_data)

# create an empty container to catch the column info text
election_data_colinfo <- rep(NA, times=numcols)

# for every number x between 1 and however many columns election_data has:
for (x in 1:numcols){
  # to the x'th element of 
  election_data_colinfo[x] <- attributes(election_data[[x]])$label
}

# now make a df w each row as the name and description of an election_data col
election_dictionary <- data.frame(
  colname  = names(election_data), 
  colinfo  = election_data_colinfo
)

# check out the first 20 -- not bad, eh?
head(election_dictionary, n=20)
```

**note**: here we're using a `for`-loop, which is the first **control structure** we've seen. Depending on class interest, we can go deeper into looping next week. It's an important topic, but you can do a lot of stuff in R 
without ever writing loops. 

**exercise**: google "for-loops vs apply family in R", read up about the function `lapply()`, and then re-write the loop above as a vectorized "loop" with `sapply()` or `lapply()`. 

**exercise (for those interested in SPSS)**: find an SPSS dataset and read it in. 

#### 3.4 simulating data

If we don't have actual data on a topic but still want to explore it quantitatively, a good option is to use randomly (but systematically!) **simulate** some data. Here's an example:

```{r}
# what's the probability that two of the people here have the same bday?!
# here's one strategy we could use: 
# get a vector of days of the year 
days <- seq(as.Date("2017-01-01"), as.Date("2017-12-31"), "days")

# define a df with 11 people, randomly assigning birthdays
birthday <- data.frame(
  # create 11 "people"
  person = paste0("person_", 1:11),
  # sample from days with replacement to assign birthdays
  bday   = sample(days, size=11, replace=TRUE)
)

# write a statement that'll be true iff two ppl have the same bday
length(unique(birthday$bday)) < nrow(birthday)
```

**exercise**: whether two people have the same birthday will vary each time 
you run the above, because we're randomly sampling from days. Conceptually (i.e. without writing any code), how would we answer the question of how likely it is 
for two people of the 11 to share a birthday? Now, practically (i.e. by writing some code), how likely is it? (*hint*: use a `for`-loop -- ask me if you're interested)

**exercise**: make a plot that represents the change in probability of two 
people having the same birthday as the size of the class increases. 

```{r}
# define some parameters
numsims <- 100
numppl  <- 11
# make a container to hold the simulation results
container <- rep(NA, times=numsims)
# loop over 1,2,...,numsims and generate numppl-many birthdays
for (x in 1:numsims){
  dat <- sample(days, size=numppl, replace=TRUE)
  # for each iteration, assign TRUE to the container element if we have a match
  container[x] <- length(unique(dat)) < length(dat)
}
# now get the proportion of sims where there's a common bday
sum(container==TRUE) / length(container)
# make a quick plot to see the results
ggplot2::qplot(container)
```


#### 3.5 cleaning up a dataset and then writing (saving) it

Say we want to introduce info about the region of the manufacturer of each make/model in the `mtcars` dataset. One approach we might use: first, list all the manufacturers in the dataset, organizing them by where the maker is from (here I'm just relying on pre-existing knowledge). Then make a data frame consisting of all the unique manufacterers (`mfr`), and the regions associated with them. 

Here's the first step:

```{r}
mfr_NA   <- c("Hornet", "Valiant", "Duster")           # unknown  manufacturer
mfr_asia <- c("Mazda", "Datsun", "Honda", "Toyota")    # asian    manufacturer
mfr_usa  <- c("Cadillac","Lincoln","Chrysler","Dodge", # american manufacturer
              "AMC","Chevrolet","Pontiac","Ford")      # european manufacturer
mfr_euro <- c("Mercedes", "Fiat", "Porsche", "Lotus", 
              "Ferrari", "Maserati", "Volvo")
```

These vectors have all the info necessary to encode manufacturer region for the `mtcars` dataset. Now how should we integrate this into the dataset?! There's more than one way we could do this, but here's one. 

Make the four `mfr_*` vectors into a single object...But how should we do *that*? A natural instinct might be to make each of them a column of a data frame...

```{r}
# but if we try this we'll get an error -- why?
# car_regions <- data.frame(mfr_NA, mfr_asia, mfr_usa, mfr_euro)
```

A good way to represent this information would be as a data frame with two columns: one listing the manufacturer, and the other listing the region. Here's how we could create such a data frame:

```{r}
# make a data frame assigning regions to car types
car_regions <- data.frame(
  # the mfr_* vectors strung together
  make   = c(mfr_NA, mfr_asia, mfr_usa, mfr_euro), 
  # assign regions to manufacturers, based on the mfr_* vectors and 'make'
  # the idea is to repeat the label for each value in the corresponding vector
  region = c(rep(NA,    length(mfr_NA)),   rep("asia", length(mfr_asia)), 
             rep("usa", length(mfr_usa)),  rep("euro", length(mfr_euro))),
  # since we know we'll be joining this with another df, don't use factors
  stringsAsFactors=FALSE
)

```

<!-- # match(c("Valiant","Honda"), table=c(mfr_NA, mfr_asia, mfr_usa, mfr_euro)) -->

**exercise**: what's another way we could have associated manufacturers with regions? (*hint*: it's reminiscent of what we do with `gear_lookup` below)


"number of gears" (`mtcars$gear`) is technically a number, but practically speaking it's kind of like a category (albeit ordered): there's only a few possible values, and there's no reason to think the difference between 3 and 4 gears is "comparable" to the difference between 4 and 5 gears. To avoid accidentally computing on `mtcars$gear` as if it were numeric, let's recode it as character. Here is a nice, concise way to do this: 

```{r}
# "number of gears" is a category but currently coded as numeric

# make a "lookup table" that associates values of gear with the labels we want 
gear_lookup <- c(three=3, four=4, five=5)

# now combine names(), match(), and [] to recode the values how we want them
mtcars$gear <- names(gear_lookup[match(mtcars$gear, gear_lookup)])
```

**note**: since we manipulated `mtcars`, now it shows up in the environment pane in R Studio :)


Okay, now let's do some other stuff to the data and then save the cleaned up version. 

The following block of code illustrates the utility of combining `dplyr::` data manipulation operations with the forward pipe operator `%>%` from `magrittr::`. 

**note**: I don't expect that you guys will be able to read all of this next block just yet. With some practice, you will. And once you see exactly what's going on,  then you'll be well on your way to writing nice, pretty, precise, concise, etc. data processing workflows. 

**note**: See [the first appendix](#appendix_pipe) for a quick description of how `magrittr::`'s forward pipe operator `%>%` works. 

```{r}
# the variable 'mtcars_clean' will hold the result of piping mtcars
# into the chain mutate() %>% select() %>% rename()
mtcars_clean <- mtcars %>% 
  mutate(
    car       = row.names(mtcars),                        # create 'car' column
    qsec      = round(qsec),                              # round qm time
    mpg       = round(mpg),                               # round mpg
    wt        = wt * 1000,                                # get weight in lbs
    am        = ifelse(am==0, "manual", "auto"),          # code as char
    musclecar = cyl >= 6 & hp > 200 & qsec < 16           # define a muscle car
  ) %>% 
  select(
    car, am, gear, musclecar, cyl, 
    hp, qsec, gear, wt, mpg
  ) %>% 
  rename(
    horsepower=hp, cylinders=cyl, qm_time=qsec, 
    num_gears=gear, lbs=wt, transmission=am
  )
```

Now the dataset is cleaned up to our liking and now we want to use the cleaned up vesion as our official version of record (or share it with ppl). We'll save it 
in a few different formats as a demo. 

**note**: Unless you have a very good reason to do otherwise (e.g. your boss demands .xls format), it's always best to write data frames to disk in .csv format. .csv is closest thing to a universal format for rectangular datasets. One exception is: if your dataset is very large (anything above ~ half a gig), it makes sense to save it in a compressed format such as `.rda`/`.RData`, so that you can load it quickly in the future. 

```{r}
# write as .csv (the default strategy)
write.csv(mtcars_clean, file="mtcars_clean.csv", row.names=FALSE)

# ...

# then read it back in later
mtcars_csv <- read.csv("mtcars_clean.csv")
rm(mtcars_csv)
```

**exercise**: try writing and then reading in `mtcars_clean`, once with `row.names=FALSE`, and once without it. What's the difference? Why might we want to specify `row.names=FALSE`?

```{r}
# write as .rda (a compressed R data file -- can include multiple objects)
save(mtcars_clean, file="mtcars_clean.rda")

# ...

# then we could use load() to read it back in later
# load("mtcars_clean.rda")
```

**note**: sometimes it's useful to save a "snapshot" of whatever you're working on, so that you can shut things down and then come back later and start where you left off. You can save your entire R workspace by using `save.image(file="my/desired/location/my_desired_filename.RData")`, and then bring it back up with `load()`. I prefer not to use `load()` in the middle of a session though, so we won't use it here. 

```{r}
# we can save a dataset in stata format
write_dta(mtcars_clean, path="mtcars_stata.dta", version=14)

# ...

# and then read it back in later
mtcars_stata <- read_dta("mtcars_stata.dta")
```

Aaaand finally, for our old friend excel, we'll look at the `rio::` package (for "R, i/o"). 



Check out [the vignette](https://cran.r-project.org/web/packages/rio/vignettes/rio.html) for the `rio::` package -- you'll see examples of how you can read and save datasets in various formats, and how you can convert between different filetypes too. Here's an example from the vignette, which I imagine could be quite useful for many of us working at NORC. (check out the output file in excel!)

```{r}
# you'll get a message w instructions for installing some suggested packages -- 
# i recommend following them 
library("rio")

# export to sheets of an Excel workbook
export(list(mtcars = mtcars, iris = iris), "multi.xlsx")
```


<br>

### 4. WTF why can't I read in this dataset right?!?!

<hr style="height:1px; background-color:lightgray; color:lightgray;">

When you're starting out, reading in an external dataset doesnt always go like you want it to...Sometimes there's invisible half-empty columns created by excel, and that breaks the "all columns must have the same length" rule for data frames. Other times, the dataset will use a weird or confusing quoting convention or non-standard separator, and that'll cause problems. **This can be extremely frustrating**, but with practice you'll learn that there's really only a few possible sources of problems for reading in plain-text data (I can't speak for other formats, unfortunately). 

**note**: I'd consider the "standard" separators to be `,` (comma), `\t` (tab), and maybe `|` (pipe). Sometimes you'll see a delimited plain-text files with extensions other than ".csv" -- some examples are ".txt" (for "text") and ".tsv" (for "tab-separated values"). These can be read in with `read.table()`, which is actually what's happening under the hood anyway when you use `read.csv()`. 

This kind of stuff really has to be learned on a case-by-case basis. So here's a list of tips for problems reading in data files:

#### common pitfalls + sources of errors/warnings

- comma versus tab separator (specify in `sep=` argument of `read.csv()`)
- misalignment of headers + values (check out `fill=` and `skip=` arguments) 
- empty cells (again, `fill=` and `skip=`)
- quoting + separators inside quoted vals (check out `quote=` and `sep=`)
- weird comment characters (check out `comment.char=`)
- directories! (you can use `"../"` to go up a dir level, and `"folder/"` to look into a folder, all in the main argument of the read call -- for example the following looks up two levels, then looks into a folder called "data/" to get the file: `read.csv("../../data/boosh.csv")`)

#### the most dangerous error!
The most dangerous error is the one that R couldn't possibly tell you about, because it's only an "error" in the sense that there was something wrong in the file you're reading in. The two most common cases of this in my experience are: 

**LEADING/TRAILING SPACES IN VALUES AND/OR COLUMN NAMES!**

  - `"dog"` vs `"dog "` vs `" dog"` vs `" dog "` all seem the same to humans (*especially* if you're using Excel), but they are **not** the same to a computer.
  - thus it's best practice to completely eliminate spaces from column names. This has the added benefit that if you do this, you can use the convenient dollar-sign syntax for column selection (**exercise**: what happens if you use `$` on a column that has a space in the name? what about one with a special character?)

```{r include=FALSE}
names(mtcars_clean) <- c("waowwie! @ # <o_0> ", names(mtcars_clean)[1:(ncol(mtcars_clean)-1)])
```


**NON-STANDARD SYMBOLS FOR "MISSINGS" OR COMMENT CHARACTERS**

  - sometimes data will have missing values coded as `99` or `-1` or something else. Since there's inconsistency across software, this can lead to the most dangerous kind of error -- **the silent error**.
  - check what the comment character is of the software that created the dataset, it could be that there's comments in the data file, but R is trying to read them as rows. If you know this to be the case, ask yourself how you should specify `comment.char`, `na.strings`, and `skip` arguments in the `read.*()` command you use.

#### useful troubleshooting strategies

- look at the data file in a plain text editor (**I'd highly recommend getting Sublime Text** -- you can use the free version forever, it'll just ask you if you want to buy a license every once in a while)
- read the documentation of `readxl::`, `haven::`, and `rio::`
- use the R Studio point-and-click data import interface (*but* save the code after!)
- re-open file in excel and (re-)export, this time specifying the options you want when you save as .csv.


<br>

### 5. Once everything's how we want it to be...

<hr style="height:1px; background-color:lightgray; color:lightgray;">

Let's clean up the workspace then read in `mtcars_clean` again. 

```{r}
# read it in
dat <- read.csv("mtcars_clean.csv")

knitr::kable(head(dat, 5))
```

Here's a bunch of code that does a bunch of useful stuff. If you understand how this code works, you'll be able to apply it to your own unique situation.

```{r}
# now manipulate it in a bunch of ways

# some ideas to start with?
#   - aggregation  
#   - subsetting
#   - grouping vars (dplyr)
#   - summary statistics
#   - contingency tables
#   - diagnostic plots
#   - modeling...


# [FILL IN LIVE DURING CLASS]
# [FILL IN LIVE DURING CLASS]
# [FILL IN LIVE DURING CLASS]

```





<br>

### Next Week

<hr style="height:1px; background-color:lightgray; color:lightgray;">

- we look through everyone's datasets and discuss any issues that came up
- wide- vs long-format data, reshaping data, the concept of "tidy data"
- visualizing a dataset as a class (type-along)
- visualizing your own dataset with base graphics and `ggplot2::`



<br>

### Appendix: the forward pipe operator `%>%` from `magrittr::` {#appendix_pipe}

<hr style="height:1px; background-color:lightgray; color:lightgray;">

Most R commands consist of a function applied to one or more arguments (potentially assigning the result to a variable). In the case where there's only one argument, it can be nice to use the forward pipe operator `%>%`. This is part of a family of similar operators defined in the `magrittr::` package, and is made use of heavily in modern `dplyr::` data processing workflows. 

It's not as scary as it looks: `x %>% f()` is equivalent to `f(x)`. What's nice about this is that you can make "pipe-chains" when you want to apply a sequence of functions to a single object (`dplyr::`'s functions are designed for exactly this). Forward pipe-chains have the following shape:

> `x %>% f() %>% g() %>% h() %>% z()`

Which is equivalent to:

> `z(h(g(f(x))))`, 

except that the first one is easier to read (*or will become easier to read if you start using it regularly*). Assuming we want to save the result of `x` applied to `f()` through `z()`, we can just assign the whole chain to a variable. Here's a little example where given the schema above, `x` is `chars`, and `f()` and `g()` are `unique()` and `length()`. 

```{r}
chars <- sample(letters, size=20, replace=TRUE)

# we could write
numUnique <- length(unique(chars))
numUnique
```

```{r}
# or equivalently:
numUnique <- chars %>% unique() %>% length()
numUnique
```


After reading this, go back and look at the `mtcars` pipe-chain above. Make any more sense now? The more you use pipes, the more natural it will feel, and the weirder it will feel to use nested function calls like `length(unique())`. 



<br>

### Appendix: writing functions {#appendix_func}

<hr style="height:1px; background-color:lightgray; color:lightgray;">


The more you use R, the more things you'll realize you could be doing 
in a waaaay more efficient manner. Learning to write your own functions 
is a crucial step in making your workflow and data processing pipelines more 
efficient and less headache-inducing. 

A simple example: often we have something that's coded as a factor or a number, 
and we'd rather have it coded as a character. 

```{r}
thing1 <- factor(rep(1:3, 5), labels=c("categoryA", "categoryB", "categoryC"))

as.character(thing1)
```

Imagine we had several objects like `thing1` and wanted to apply `as.character()` 
to each of them (e.g. because they all have the same values, but the underlying factor values are a non-uniform mess). 

```{r}
thing1 <- factor(rep(1:3, 5), labels=c("catA", "catB", "catC"))
thing2 <- factor(rep(4:6, 5), labels=c("catA", "catB", "catC"))
thing3 <- factor(rep(3:5, 5), labels=c("catA", "catB", "catC"))
thing4 <- factor(rep(2:4, 5), labels=c("catA", "catB", "catC"))
thing5 <- factor(rep(3:1, 5), labels=c("catA", "catB", "catC"))
```

We could type out `as.character()` each time: 

```{r eval=FALSE}
thing1 <- as.character(thing1)
thing2 <- as.character(thing2)
# ...
```

Or we could write a function that basically just abbreviates it. That'll save us keystrokes.

```{r}
# a quick function to save us keystrokes
ac <- function(x){
  as.character(x)
}

thing1 <- ac(thing1)
thing2 <- ac(thing2)
# ...
```

Another example:

```{r}
# saves us even more keystrokes
lu <- function(x){
  length(unique(x))
}

lu(thing3)
lu(thing4)
lu(thing5)
```

**exercise**: rewrite the body of the `lu()` function as a forward pipe-chain with `%>%` (also, it's good practice to put `require("magrittr")` or `require("dplyr")` at the beginning of the function body since we'll need one of those packages to use the pipe).

So what can writing functions do for you? 
Here's a function that does some useful stuff. Think of it as breaking data 
analysis or summary into two pieces: when you define your main function, you're 
defining **the data analysis routine**. When you actually apply the function to 
your data, you're **executing the analysis**. 


```{r}
# define analysis routine
custom_summary <- function(df, group_col, measure_col){
  require("dplyr"); require("ggplot2")
  
  # make a lil df with just the relevant cols
  df <- data.frame(group_col=df[[group_col]], measure_col=df[[measure_col]])

  # make a table of the mean and sd of measure_col for each value of group_col
  out_table <- df %>% group_by(group_col) %>% summarize(
    avg = mean(measure_col, na.rm=TRUE),
    sd  = sd(measure_col, na.rm=TRUE)     # ... more calculations
  ) %>% data.frame()
  
  # make a plot the avg of measure_col for each value of group_col
  out_plot <- ggplot(out_table, aes(x=group_col, y=avg)) +
    geom_bar(stat="identity") +
    geom_errorbar(aes(ymin=avg-sd, ymax=avg+sd, width=.25)) +
    labs(x=group_col, y=paste0("mean of ", measure_col, ", +/- sd"), 
         title=paste0("average ", measure_col, " by ", group_col))
  
  # now combine the table (out_table) and the plot (out_plot) 
  # we're using a list here because these objects are very different!
  # we can also use the $ syntax on the output, which is why we give 
  # both of the slots in 'out' a name ('table' and 'plot')
  out <- list(table=out_table, plot=out_plot)
  
  # then return 'out' as the value of the function
  # (if you don't specify return value, the default is the final printed value)
  return(out)
}
```

We can apply `custom_summary()` to `mtcars` in a number of ways. 
Summarize `mtcars$mpg` for each value of `mtcars$gear` using 
`custom_summary()`, and assign the result to the variable `mpg_by_gear`. 

```{r}
mpg_by_gear <- custom_summary(df=mtcars, group_col="gear", measure_col="mpg")
```

Now we can look at the components of the result that we decided to return 
when we defined `custom_summary()`:

```{r}
# print a table
knitr::kable(mpg_by_gear$table)
```

Now the plot: 

```{r}
# display the plot
mpg_by_gear$plot
```

We can repeat the process with any data frame and any combination of variables. Play around with this for a while and some ideas might come to your mind about how we could include other useful information in the return value of 
`custom_summary()`.

```{r}
mpg_by_cyl <- custom_summary(df=mtcars, group_col="cyl", measure_col="mpg")

# ...
```

**exercise**: use our function to summarize mpg by cyl and a couple other variables. How could we apply `custom_summary()` to `iris` to understand some aspect of that dataset?

**exercise (caution -- this one might not be worth the time)**: why does the plot have "gear" on the axis title and x-axis label, but the table ends up with "group_col"? Both were specified with `group_col` in the definition of `custom_summary()`, so why don't they display the same? (*hint*: read [the chapter on non-standard evaluation](LINK) from Wickham's *Advanced R*)


<hr><hr>
<br><br>



<link rel="stylesheet" type="text/css"
href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,400i,700">

<link href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,400,500" rel="stylesheet">

  <style>
body {
  padding: 10px;
  font-size: 12pt;
  font-family: 'Open Sans', sans-serif;
}

h1 { 
  font-size: 20px;
  color: DarkGreen;
  font-weight: bold;
}

h2 { 
    font-size: 16px;
    color: green;
}

h3 { 
  font-size: 24px;
  color: green;
  font-weight: bold;
}

h4 { 
  font-size: 18px;
  color: green;
  font-weight: bolder;
  padding-top: 30px;
}

li {
  padding: 3px;
}

code {
  font-family: 'Roboto Mono', monospace;
  font-size: 14px;
}

pre {
  font-family: 'Roboto Mono', monospace;
  font-size: 14px;
}

p {
  margin-top: 30px;
  margin-bottom: 15px;
}

</style>



<!-- END OF DOCUMENT IS HERE -->

